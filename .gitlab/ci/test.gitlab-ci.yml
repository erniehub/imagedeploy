include:
  - template: Security/Secret-Detection.gitlab-ci.yml

.test-job: &test-job
  stage: test
  image: "$BUILD_IMAGE_NAME"
  parallel:
    matrix:
      - K3S_VERSION:
        - v0.8.1 # kube 1.14.6, see https://github.com/rancher/k3s/releases/tag/v0.8.1
        - v0.9.1 # kube 1.15.4, see https://github.com/rancher/k3s/releases/tag/v0.9.1 and https://github.com/rancher/k3s/releases/tag/v0.9.0
        - v1.16.15-k3s1
        - v1.17.13-k3s1
        - v1.18.10-k3s1
        - v1.19.3-k3s1
  services:
    - name: registry.gitlab.com/gitlab-org/cluster-integration/test-utils/k3s-gitlab-ci/releases/${K3S_VERSION}
      alias: k3s
  before_script:
    - curl k3s:8081?service=k3s > k3s.yaml
    - export KUBECONFIG=$(pwd)/k3s.yaml
    - kubectl version
    - kubectl cluster-info
    - |
        if [[ "$K3S_VERSION" < "v1" ]]; then
          kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
          kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
        fi
    - |
        function expected_error() {
          echo "Expected error but exited with $?, failing build!"
          exit 1
        }

        function failed_as_expected() {
          echo "Failed as expected and exited with $?"
        }

test-dependencies:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
  script:
    - helm version --client
    - kubectl version --client

test-kube-domain:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    KUBE_INGRESS_BASE_DOMAIN: example.com
  script:
    - auto-deploy check_kube_domain

test-kube-domain-legacy:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    AUTO_DEVOPS_DOMAIN: example.com
  script:
    - auto-deploy check_kube_domain && expected_error || failed_as_expected

test-kube-domain_error:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
  script:
    - auto-deploy check_kube_domain && expected_error || failed_as_expected

test-download-chart:
  <<: *test-job
  script:
    - auto-deploy download_chart
    - ./test/verify-chart-version 2

test-deploy-name:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
  script:
    - name=$(auto-deploy deploy_name "stable")
    - |
      if [[ $name != "production" ]]; then
        echo "$name should equal 'production'"
        exit 1
      fi
    - name=$(auto-deploy deploy_name "canary")
    - |
      if [[ $name != "production-canary" ]]; then
        echo "$name should equal 'production-canary'"
        exit 1
      fi

test-get-replicas:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
    TRACK: stable
  script:
    # When `REPLICAS` variable is not specified
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 1 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `REPLICAS` variable is specified
    - export REPLICAS="2"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 2 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `<env>_REPLICAS` variable is specified
    - export PRODUCTION_REPLICAS="3"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 3 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `<track>_<env>_REPLICAS` variable is specified
    - export STABLE_PRODUCTION_REPLICAS="4"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 4 ]]; then echo "Unexpected replicas"; exit 1; fi

test-get-replicas-canary:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
    TRACK: canary
  script:
    # When `REPLICAS` variable is not specified
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 1 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `REPLICAS` variable is specified
    - export REPLICAS="2"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 2 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `<env>_REPLICAS` variable is specified
    - export PRODUCTION_REPLICAS="3"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 3 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `<track>_<env>_REPLICAS` variable is specified
    - export CANARY_PRODUCTION_REPLICAS="4"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 4 ]]; then echo "Unexpected replicas"; exit 1; fi

test-get-replicas-zero:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
    REPLICAS: "0"
  script:
    - replicas=$(auto-deploy get_replicas "stable")
    - |
      if [[ $replicas != 0 ]]; then
        echo "$replicas should equal 0, as requested"
        exit 1
      fi

test-ensure-namespace:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    KUBE_NAMESPACE: project-123456
  script:
    - auto-deploy ensure_namespace

test-initialize-tiller:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    KUBE_NAMESPACE: default
  script:
    - auto-deploy initialize_tiller | grep "Helm 3 does not have Tiller"

test-create-secret:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    KUBE_NAMESPACE: default
    CI_REGISTRY: example.com
    CI_DEPLOY_USER: ci-deploy-user
    CI_DEPLOY_PASSWORD: ci-deploy-password
    GITLAB_USER_EMAIL: user@example.com
    CI_PROJECT_VISIBILITY: private
  script:
    - auto-deploy create_secret
    - kubectl get secret "gitlab-registry-${CI_PROJECT_PATH_SLUG}" -n $KUBE_NAMESPACE

test-create-secret-public-project:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_PROJECT_VISIBILITY: public
    KUBE_NAMESPACE: default
  script:
    - auto-deploy create_secret
    - kubectl get secret "gitlab-registry-${CI_PROJECT_PATH_SLUG}" -n $KUBE_NAMESPACE && expected_error || failed_as_expected

test-persist-environment-url:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_URL: review-app.example.com
  script:
    - auto-deploy persist_environment_url
    - grep review-app.example.com environment_url.txt

test-install-postgres:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
    KUBE_NAMESPACE: default
    AUTO_DEVOPS_POSTGRES_CHANNEL: 2
    POSTGRES_USER: user
    POSTGRES_PASSWORD: testing-password
    POSTGRES_DB: $CI_ENVIRONMENT_SLUG
  script:
    - mkdir -p .gitlab
    - "echo 'custom_key: custom_value' > .gitlab/auto-deploy-postgres-values.yaml"
    - auto-deploy download_chart
    - auto-deploy install_postgresql
    - helm get values production-postgresql --namespace "$KUBE_NAMESPACE" --output json | grep -q '"custom_key":"custom_value"' || exit 1
    - kubectl get statefulset production-postgresql -n $KUBE_NAMESPACE

test-deploy:
  <<: *test-job
  variables: &deploy-variables
    CI_APPLICATION_REPOSITORY: "registry.gitlab.com/gitlab-org/cluster-integration/auto-deploy-image/auto-build-image-with-psql"
    CI_APPLICATION_TAG: "5d248f6fa69a"
    CI_ENVIRONMENT_SLUG: production
    CI_ENVIRONMENT_URL: example.com
    ADDITIONAL_HOSTS: '*.example.com, extra.host.com'
    CI_PROJECT_PATH_SLUG: "gitlab-org/cluster-integration/auto-build-image"
    CI_PROJECT_VISIBILITY: public
    KUBE_NAMESPACE: default
    KUBE_INGRESS_BASE_DOMAIN: example.com
    ROLLOUT_RESOURCE_TYPE: deployment
    POSTGRES_USER: user
    POSTGRES_PASSWORD: testing-password
    POSTGRES_ENABLED: "true"
    POSTGRES_DB: $CI_ENVIRONMENT_SLUG
    HELM_HOST: "localhost:44134"
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    - helm get all production
    - helm get values production --output json | grep "postgres://user:testing-password@production-postgresql:5432/production"
    - ./test/verify-deployment-database production postgresql

test-deploy-postgres-disabled:
  extends: test-deploy
  variables:
    POSTGRES_ENABLED: "false"
  script:
    - auto-deploy initialize_tiller
    - auto-deploy download_chart
    - auto-deploy deploy
    - helm get production
    - helm list > releases.txt
    - if grep -q "postgres" releases.txt; then echo "postgresql should not be installed"; exit 1; fi

test-deploy-atomic:
  extends: test-deploy
  variables:
    POSTGRES_ENABLED: "false"
    KUBE_INGRESS_BASE_DOMAIN: ""
  script:
    - auto-deploy download_chart
    - auto-deploy deploy && exit 1 || echo "First deployment failed as expected"
    # second deploy should succeed, there should be no first release
    - if [[ -n "$(helm ls -q)" ]]; then exit 1; fi
    - export KUBE_INGRESS_BASE_DOMAIN=example.com
    - auto-deploy deploy

test-deploy-non-atomic:
  extends: test-deploy
  variables:
    POSTGRES_ENABLED: "false"
    KUBE_INGRESS_BASE_DOMAIN: ""
    AUTO_DEVOPS_ATOMIC_RELEASE: "false"
  script:
    - auto-deploy initialize_tiller
    - auto-deploy download_chart
    - auto-deploy deploy && exit 1 || echo "First deployment failed as expected"
    # second deploy should succeed on top of the first failed release
    - if [[ -z "$(helm ls -q)" ]]; then exit 1; fi
    - export KUBE_INGRESS_BASE_DOMAIN=example.com
    - auto-deploy deploy

test-deploy-debug:
  extends: test-deploy
  variables:
    AUTO_DEVOPS_DEPLOY_DEBUG: "1"
  script:
    - auto-deploy download_chart
    - auto-deploy deploy

test-deploy-when-stable-chart-repository-is-unreachable:
  extends: test-deploy
  variables:
    <<: *deploy-variables
  script:
    - echo "127.0.0.1 kubernetes-charts.storage.googleapis.com" >> /etc/hosts
    - auto-deploy initialize_tiller
    - auto-deploy download_chart
    - auto-deploy deploy

test-scale-does-not-create-old-postgres:
  extends: test-deploy
  script:
    - auto-deploy initialize_tiller
    - auto-deploy download_chart
    - auto-deploy deploy
    - auto-deploy scale
    - exist=$(auto-deploy check_old_postgres_exist)
    - |
      if [[ "$exist" != "false" ]]; then
        echo "Old Postgres should not exist"
        exit 1
      fi

test-show-warning-for-legacy-in-cluster-postgresql:
  extends: test-deploy
  script:
    # Create a release/deployment
    - auto-deploy download_chart
    - auto-deploy deploy
    # Forcibly update the release that a legacy in-cluster postgresql exists in it
    - helm upgrade --reuse-values --wait --set postgresql.enabled="true" --namespace="$KUBE_NAMESPACE" "${CI_ENVIRONMENT_SLUG}" chart/
    - helm get values --namespace "$KUBE_NAMESPACE" --output json "${CI_ENVIRONMENT_SLUG}"
    # It should see an error when the deployment is upgraded
    - auto-deploy deploy| tee deploy.log || true
    - grep -q "Detected an existing PostgreSQL database" deploy.log || exit 1

test-deploy-canary:
  extends: test-deploy
  script:
    - auto-deploy download_chart
    - auto-deploy deploy canary
    - helm get all production-canary
    # It should have Canary Ingress
    - kubectl describe ingress production-canary-auto-deploy -n $KUBE_NAMESPACE > ingress.spec
    - grep -q 'nginx.ingress.kubernetes.io/canary:.*true' ingress.spec || exit 1

test-deploy-modsecurity:
  extends: test-deploy
  variables:
    <<: *deploy-variables
    AUTO_DEVOPS_MODSECURITY_SEC_RULE_ENGINE: "On"
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    - $([[ $(kubectl get ingress production-auto-deploy -n $KUBE_NAMESPACE --no-headers=true -o custom-columns=:"metadata.annotations.nginx\.ingress\.kubernetes\.io/modsecurity-snippet") != "<none>" ]])

test-create-application-secret:
  <<: *test-job
  variables:
    KUBE_NAMESPACE: default
    CI_ENVIRONMENT_SLUG: production
    K8S_SECRET_CODE: 12345
    K8S_SECRET_CODE_MULTILINE: "12345
    NEW LINE"
  script:
    - auto-deploy create_application_secret "stable"
    - kubectl get secrets -n $KUBE_NAMESPACE
    - kubectl get secrets production-secret -n $KUBE_NAMESPACE
    - ./test/verify-application-secret

test-delete:
  extends: test-deploy
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    - helm get all production
    - auto-deploy delete
    - helm get all production && expected_error || failed_as_expected

test-delete-postgresql:
  extends: test-deploy
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    - helm get all production
    - helm get all production-postgresql
    - pvc_before_delete=$(kubectl -n $KUBE_NAMESPACE get pvc -l release=production-postgresql)
    - if [[ -z "$pvc_before_delete" ]]; then "expected to find a postgresql pvc"; exit 1; fi
    - auto-deploy delete
    - helm get all production && expected_error || failed_as_expected
    - helm get all production-postgresql && expected_error || failed_as_expected
    - pvc_after_delete=$(kubectl -n $KUBE_NAMESPACE get pvc -l release=production-postgresql)
    - if [[ -n "$pvc_after_delete" ]]; then echo "no postgresql pvc should be present"; exit 1; fi

test-delete-canary-postgresql:
  extends: test-deploy
  script:
    - auto-deploy download_chart
    - auto-deploy deploy canary
    - helm get all production-canary
    - auto-deploy deploy
    - helm get all production
    - helm get all production-postgresql
    - auto-deploy delete canary
    - helm get all production-canary && expected_error || failed_as_expected
    - helm get all production
    - helm get all production-postgresql

test-chart-major-version-upgrade:
  extends: test-deploy
  script:
    - auto-deploy initialize_tiller
    # Copying bundled chart from local storage and the deployment should succeed
    - auto-deploy download_chart
    - auto-deploy deploy
    # Modifying the chart version and the deployment should fail
    - "sed -i 's/version:.*/version: 10.0.0/g' chart/Chart.yaml"
    - cat chart/Chart.yaml
    - auto-deploy deploy| tee deploy.log || true
    - grep -q "Detected a major version difference" deploy.log || exit 1
    # Force deploy with the AUTO_DEVOPS_FORCE_DEPLOY option and the deployment should succeed
    - export AUTO_DEVOPS_FORCE_DEPLOY_V10=true
    - auto-deploy deploy| tee deploy.log
    - grep -q "allowed to force deploy" deploy.log || exit 1

test-upgrade-from-helm2-fails:
  image: docker:19.03.12
  services:
    - docker:19.03.12-dind
    - name: registry.gitlab.com/gitlab-org/cluster-integration/test-utils/k3s-gitlab-ci/releases/v1.16.7-k3s1
      alias: k3s
  before_script:
    - cat /etc/hosts
    - apk add curl
    # get an IP for k3s that can be accessed from within docker containers
    - K3S_IP=$(cat /etc/hosts | awk '{if ($2 == "k3s") print $1;}')
    - curl -fs k3s:8081?service="$K3S_IP" > k3s.yaml
    - export KUBECONFIG=$(pwd)/k3s.yaml
    - cat $KUBECONFIG
  script:
    # use an env-file to forward variables to the containers
    - |
      echo 'CI_APPLICATION_REPOSITORY=registry.gitlab.com/gitlab-org/cluster-integration/auto-deploy-image/auto-build-image-with-psql
        CI_APPLICATION_TAG=5d248f6fa69a
        CI_ENVIRONMENT_SLUG=production
        CI_ENVIRONMENT_URL=example.com
        CI_PROJECT_PATH_SLUG=gitlab-org/cluster-integration/auto-build-image
        CI_PROJECT_ID=1
        CI_PROJECT_VISIBILITY=public
        KUBE_NAMESPACE=default
        KUBE_INGRESS_BASE_DOMAIN=example.com
        ROLLOUT_RESOURCE_TYPE=deployment
        POSTGRES_USER=user
        POSTGRES_PASSWORD=testing-password
        POSTGRES_ENABLED=true
        POSTGRES_DB=production
        HELM_HOST=localhost:44134
        KUBECONFIG=/tmp/k3s.yaml' > /tmp/env
    # helm 2 deployment should succeed
    - |
      docker run -v $KUBECONFIG:/tmp/k3s.yaml --env-file /tmp/env registry.gitlab.com/gitlab-org/cluster-integration/auto-deploy-image:v1.1.0 \
        sh -c 'auto-deploy initialize_tiller && auto-deploy download_chart && auto-deploy deploy'
    # helm 3 deployment should fail because the deployment would overwrite an existing resource
    - |
      docker run -v $KUBECONFIG:/tmp/k3s.yaml  --env-file /tmp/env "$BUILD_IMAGE_NAME" \
        sh -c 'auto-deploy initialize_tiller && auto-deploy download_chart && auto-deploy deploy 2>&1 && exit 1 || exit 0' \
        | grep 'Error: rendered manifests contain a resource that already exists.'

rspec:
  stage: test
  image: ruby:2.7
  before_script:
    - gem install --no-document rspec
  script:
    - rspec test/rspec
