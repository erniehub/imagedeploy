.test-job: &test-job
  extends:
    - .rules:except-docs
  stage: test
  image: "$BUILD_IMAGE_NAME"
  interruptible: true
  retry: 1
  parallel:
    matrix:
      - K3S_VERSION:
        - v1.26.12-k3s1
        - v1.27.9-k3s1
        - v1.28.5-k3s1
        - v1.29.0-k3s1

  services:
    - name: registry.gitlab.com/gitlab-org/cluster-integration/test-utils/k3s-gitlab-ci/releases/${K3S_VERSION}
      alias: k3s
  before_script:
    - curl k3s:8081?service=k3s > k3s.yaml
    - export KUBECONFIG=$(pwd)/k3s.yaml
    - kubectl version
    - kubectl cluster-info
    - |
        if [[ "$K3S_VERSION" < "v1" ]]; then
          kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
          kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
        fi
    - |
        function expected_error() {
          echo "Expected error but exited with $?, failing build!"
          exit 1
        }

        function failed_as_expected() {
          echo "Failed as expected and exited with $?"
        }

test-use-kube-context:
  <<: *test-job
  variables:
    KUBE_CONTEXT: default
  script:
    # This test that any function will be properly
    # loaded. Even when calling `kubectl config --minify`
    # without a current context pre-setup
    - kubectl config unset current-context
    - kubectl config get-contexts
    - auto-deploy use_kube_context
    - context=$(kubectl config current-context)
    - |
      if [[ "$context" != "default" ]]; then
        echo "Failed to assign context"
        exit 1
      fi

test-dependencies:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
  script:
    - helm version --client
    - kubectl version --client

test-kube-domain:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    KUBE_INGRESS_BASE_DOMAIN: example.com
  script:
    - auto-deploy check_kube_domain

test-kube-domain_error:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
  script:
    - auto-deploy check_kube_domain && expected_error || failed_as_expected

test-download-chart:
  <<: *test-job
  script:
    - auto-deploy download_chart
    - ./test/verify-chart-version 2

test-download-chart-url:
  <<: *test-job
  script:
    # package the chart for the chart server
    - (cd assets && helm package auto-deploy-app)
    # install a helm chart server and serve the local chart
    - curl -LO https://get.helm.sh/chartmuseum-v0.15.0-linux-amd64.tar.gz
    - tar -xvf chartmuseum-v0.15.0-linux-amd64.tar.gz && mv linux-amd64/* .
    - chmod +x ./chartmuseum
    - ./chartmuseum --port=8080 --storage=local --storage-local-rootdir="./assets" &
    # instruct auto-deploy to use the chart server
    - export AUTO_DEVOPS_CHART_REPOSITORY_NAME=chartmuseum
    - export AUTO_DEVOPS_CHART_REPOSITORY=http://localhost:8080
    - export AUTO_DEVOPS_CHART=chartmuseum/auto-deploy-app
    - auto-deploy download_chart
    - ./test/verify-chart-version 2

test-download-protected-chart-url:
  <<: *test-job
  script:
    # package the chart for the chart server
    - (cd assets && helm package auto-deploy-app)
    # install a helm chart server and serve the local chart
    - curl -LO https://get.helm.sh/chartmuseum-v0.15.0-linux-amd64.tar.gz
    - tar -xvf chartmuseum-v0.15.0-linux-amd64.tar.gz && mv linux-amd64/* .
    - chmod +x ./chartmuseum
    - ./chartmuseum --port=8080 --storage=local --storage-local-rootdir="./assets" --basic-auth-user="user" --basic-auth-pass="pass" &
    # instruct auto-deploy to use the chart server
    - export AUTO_DEVOPS_CHART_REPOSITORY_NAME=chartmuseum
    - export AUTO_DEVOPS_CHART_REPOSITORY=http://localhost:8080
    - export AUTO_DEVOPS_CHART=chartmuseum/auto-deploy-app
    - export AUTO_DEVOPS_CHART_REPOSITORY_USERNAME=user
    - export AUTO_DEVOPS_CHART_REPOSITORY_PASSWORD=pass
    - auto-deploy download_chart
    - ./test/verify-chart-version 2

test-default-postgres-chart:
  <<: *test-job
  variables:
    POSTGRES_ENABLED: "true"
  script:
    - auto-deploy download_chart
    - (helm repo ls | grep -q 'https://raw.githubusercontent.com/bitnami/charts/eb5f9a9513d987b519f0ecd732e7031241c50328/bitnami') || exit 1

test-override-postgres-chart:
  <<: *test-job
  variables:
    POSTGRES_ENABLED: "true"
    POSTGRES_CHART_REPOSITORY: https://charts.bitnami.com/bitnami
  script:
    - auto-deploy download_chart
    - (helm repo ls | grep -q https://charts.bitnami.com/bitnami) || exit 1

test-deploy-name:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
  script:
    - name=$(auto-deploy deploy_name "stable")
    - |
      if [[ $name != "production" ]]; then
        echo "$name should equal 'production'"
        exit 1
      fi
    - name=$(auto-deploy deploy_name "canary")
    - |
      if [[ $name != "production-canary" ]]; then
        echo "$name should equal 'production-canary'"
        exit 1
      fi

test-get-replicas:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
    TRACK: stable
  script:
    # When `REPLICAS` variable is not specified
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 1 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `REPLICAS` variable is specified
    - export REPLICAS="2"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 2 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `<env>_REPLICAS` variable is specified
    - export PRODUCTION_REPLICAS="3"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 3 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `<track>_<env>_REPLICAS` variable is specified
    - export STABLE_PRODUCTION_REPLICAS="4"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 4 ]]; then echo "Unexpected replicas"; exit 1; fi

test-get-replicas-canary:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
    TRACK: canary
  script:
    # When `REPLICAS` variable is not specified
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 1 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `REPLICAS` variable is specified
    - export REPLICAS="2"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 2 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `<env>_REPLICAS` variable is specified
    - export PRODUCTION_REPLICAS="3"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 3 ]]; then echo "Unexpected replicas"; exit 1; fi
    # When `<track>_<env>_REPLICAS` variable is specified
    - export CANARY_PRODUCTION_REPLICAS="4"
    - replicas=$(auto-deploy get_replicas ${TRACK})
    - if [[ $replicas != 4 ]]; then echo "Unexpected replicas"; exit 1; fi

test-get-replicas-zero:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_SLUG: production
    REPLICAS: "0"
  script:
    - replicas=$(auto-deploy get_replicas "stable")
    - |
      if [[ $replicas != 0 ]]; then
        echo "$replicas should equal 0, as requested"
        exit 1
      fi

test-ensure-namespace:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    KUBE_NAMESPACE: project-123456
  script:
    - auto-deploy ensure_namespace

test-initialize-tiller:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    KUBE_NAMESPACE: default
  script:
    - auto-deploy initialize_tiller | grep "Helm 3 does not have Tiller"

test-create-secret:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    EXPECTED_NAMESPACE: default
    CI_REGISTRY: example.com
    CI_DEPLOY_USER: ci-deploy-user
    CI_DEPLOY_PASSWORD: ci-deploy-password
    GITLAB_USER_EMAIL: user@example.com
    CI_PROJECT_VISIBILITY: private
  script:
    - auto-deploy create_secret
    - kubectl get secret "gitlab-registry-${CI_PROJECT_PATH_SLUG}" -n $EXPECTED_NAMESPACE

test-create-secret-public-project:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_PROJECT_VISIBILITY: public
    KUBE_NAMESPACE: default
  script:
    - auto-deploy create_secret
    - kubectl get secret "gitlab-registry-${CI_PROJECT_PATH_SLUG}" -n $EXPECTED_NAMESPACE && expected_error || failed_as_expected

test-persist-environment-url:
  <<: *test-job
  variables:
    GIT_STRATEGY: none
    CI_ENVIRONMENT_URL: review-app.example.com
  script:
    - auto-deploy persist_environment_url
    - grep review-app.example.com environment_url.txt

test-deploy:
  <<: *test-job
  variables:
    CI_APPLICATION_REPOSITORY: "registry.gitlab.com/gitlab-org/cluster-integration/auto-deploy-image/auto-build-image-with-psql"
    CI_APPLICATION_TAG: "5d248f6fa69a"
    CI_ENVIRONMENT_SLUG: production
    CI_ENVIRONMENT_URL: example.com
    CI_PROJECT_PATH_SLUG: "gitlab-org/cluster-integration/auto-build-image"
    CI_PROJECT_VISIBILITY: public
    KUBE_INGRESS_BASE_DOMAIN: example.com
    POSTGRES_ENABLED: "false"
    HELM_HOST: "localhost:44134"
    EXPECTED_NAMESPACE: default
  script:
    - auto-deploy use_kube_context
    - auto-deploy download_chart
    - auto-deploy ensure_namespace
    - auto-deploy deploy
    - helm -n "$EXPECTED_NAMESPACE" get all production

test-deploy-pdb:
  extends: test-deploy
  variables:
    HELM_UPGRADE_EXTRA_ARGS: |-
      --set podDisruptionBudget.enabled=true

test-deploy-custom-context:
  extends: test-deploy
  variables:
    KUBE_CONTEXT: default

test-deploy-custom-namespace:
  extends: test-deploy
  variables:
    KUBE_NAMESPACE: custom-namespace
    EXPECTED_NAMESPACE: custom-namespace

test-deploy-postgres-enabled:
  extends: test-deploy
  variables:
    POSTGRES_ENABLED: "true"
    POSTGRES_USER: user
    POSTGRES_PASSWORD: testing-password
    POSTGRES_DB: $CI_ENVIRONMENT_SLUG
  script:
    - auto-deploy initialize_tiller
    - auto-deploy download_chart
    - auto-deploy deploy
    - helm -n "$EXPECTED_NAMESPACE" get production
    - helm -n "$EXPECTED_NAMESPACE" get values production --output json | grep "postgres://user:testing-password@production-postgresql:5432/production"
    - ./test/verify-deployment-database production postgresql

test-deploy-atomic:
  extends: test-deploy
  variables:
    KUBE_INGRESS_BASE_DOMAIN: ""
  script:
    - auto-deploy download_chart
    - auto-deploy deploy && exit 1 || echo "First deployment failed as expected"
    # second deploy should succeed, there should be no first release
    - if [[ -n "$(helm ls -q)" ]]; then exit 1; fi
    - export KUBE_INGRESS_BASE_DOMAIN=example.com
    - auto-deploy deploy

test-deploy-non-atomic:
  extends: test-deploy
  variables:
    POSTGRES_ENABLED: "false"
    KUBE_INGRESS_BASE_DOMAIN: ""
    AUTO_DEVOPS_ATOMIC_RELEASE: "false"
  script:
    - auto-deploy initialize_tiller
    - auto-deploy download_chart
    - auto-deploy deploy && exit 1 || echo "First deployment failed as expected"
    # second deploy should succeed on top of the first failed release
    - if [[ -z "$(helm ls -q)" ]]; then exit 1; fi
    - export KUBE_INGRESS_BASE_DOMAIN=example.com
    - auto-deploy deploy

test-deploy-debug:
  extends: test-deploy
  variables:
    AUTO_DEVOPS_DEPLOY_DEBUG: "1"
  script:
    - auto-deploy download_chart
    - auto-deploy deploy

test-deploy-when-stable-chart-repository-is-unreachable:
  extends: test-deploy
  script:
    - echo "127.0.0.1 kubernetes-charts.storage.googleapis.com" >> /etc/hosts
    - auto-deploy initialize_tiller
    - auto-deploy download_chart
    - auto-deploy deploy

test-scale-does-not-create-old-postgres:
  extends: test-deploy
  script:
    - auto-deploy initialize_tiller
    - auto-deploy download_chart
    - auto-deploy deploy
    - auto-deploy scale
    - exist=$(auto-deploy check_old_postgres_exist)
    - |
      if [[ "$exist" != "false" ]]; then
        echo "Old Postgres should not exist"
        exit 1
      fi

test-show-warning-for-legacy-in-cluster-postgresql:
  extends: test-deploy-postgres-enabled
  script:
    # Create a release/deployment
    - auto-deploy download_chart
    - auto-deploy deploy
    # Forcibly update the release that a legacy in-cluster postgresql exists in it
    - helm upgrade --reuse-values --wait --set postgresql.enabled="true" --namespace="$EXPECTED_NAMESPACE" "${CI_ENVIRONMENT_SLUG}" chart/
    - helm get values --namespace "$EXPECTED_NAMESPACE" --output json "${CI_ENVIRONMENT_SLUG}"
    # It should see an error when the deployment is upgraded
    - auto-deploy deploy| tee deploy.log || true
    - grep -q "Detected an existing PostgreSQL database" deploy.log || exit 1

test-auto-database-url-remains-after-initial-deploy:
  extends: test-deploy-postgres-enabled
  script:
    # Create a release/deployment
    - auto-deploy download_chart
    - auto-deploy deploy
    - old=$(auto-deploy auto_database_url)
    # Simulate POSTGRES_ENABLED default change
    - unset POSTGRES_ENABLED
    - new=$(auto-deploy auto_database_url)
    - |
      if [[ "$old" != "$new" ]]; then
        echo "Database URL should not change"
        exit 1
      fi

test-auto-database-url-empty-when-disabled:
  extends: test-deploy-postgres-enabled
  script:
    # Create a release/deployment
    - auto-deploy download_chart
    - auto-deploy deploy
    # Disable postgres for new deployments
    - export POSTGRES_ENABLED=false
    - url=$(auto-deploy auto_database_url)
    - |
      if [[ -n "$url" ]]; then
        echo "Database URL should not be set"
        exit 1
      fi

test-deploy-canary:
  extends: test-deploy
  script:
    - auto-deploy download_chart
    - auto-deploy deploy canary
    - helm get all production-canary
    # It should have Canary Ingress
    - kubectl describe ingress production-canary-auto-deploy -n $EXPECTED_NAMESPACE > ingress.spec
    - grep -q 'nginx.ingress.kubernetes.io/canary:.*true' ingress.spec || exit 1

test-deploy-modsecurity:
  extends: test-deploy
  variables:
    AUTO_DEVOPS_MODSECURITY_SEC_RULE_ENGINE: "On"
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    - $([[ $(kubectl get ingress production-auto-deploy -n $EXPECTED_NAMESPACE --no-headers=true -o custom-columns=:"metadata.annotations.nginx\.ingress\.kubernetes\.io/modsecurity-snippet") != "<none>" ]])

test-create-application-secret:
  <<: *test-job
  variables:
    EXPECTED_NAMESPACE: default
    CI_ENVIRONMENT_SLUG: production
    K8S_SECRET_CODE: 12345
    K8S_SECRET_CODE_MULTILINE: "12345
    NEW LINE"
  script:
    - auto-deploy create_application_secret "stable"
    - kubectl get secrets -n $EXPECTED_NAMESPACE
    - kubectl get secrets production-secret -n $EXPECTED_NAMESPACE
    - ./test/verify-application-secret

test-install-postgres:
  extends: test-deploy-postgres-enabled
  variables:
    GIT_STRATEGY: none
  script:
    - mkdir -p .gitlab
    - "echo 'custom_key: custom_value' > .gitlab/auto-deploy-postgres-values.yaml"
    - auto-deploy download_chart
    - auto-deploy install_postgresql
    - helm get values production-postgresql --namespace "$EXPECTED_NAMESPACE" --output json | grep -q '"custom_key":"custom_value"' || exit 1
    - kubectl get statefulset production-postgresql -n $EXPECTED_NAMESPACE

test-delete:
  extends: test-deploy
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    - helm get all production
    - auto-deploy delete
    - helm get all production && expected_error || failed_as_expected

test-delete-failed:
  extends: test-deploy
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    # make sure that the helm release deployments always fails very fast
    - export HELM_UPGRADE_EXTRA_ARGS="--timeout 1s"
    - export CI_APPLICATION_REPOSITORY=this-registry-does-not-exist.test
    - export AUTO_DEVOPS_ATOMIC_RELEASE=false
    # Deployment will fail, but we wnat to continue anyway and delete the failed application
    - auto-deploy deploy || failed_as_expected
    - helm get all production
    - auto-deploy delete
    - helm get all production && expected_error || failed_as_expected

test-delete-postgresql:
  extends: test-deploy-postgres-enabled
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    - helm get all production
    - helm get all production-postgresql
    - pvc_before_delete=$(kubectl -n $EXPECTED_NAMESPACE get pvc -l release=production-postgresql)
    - if [[ -z "$pvc_before_delete" ]]; then "expected to find a postgresql pvc"; exit 1; fi
    - auto-deploy delete
    - helm get all production && expected_error || failed_as_expected
    - helm get all production-postgresql && expected_error || failed_as_expected
    - pvc_after_delete=$(kubectl -n $EXPECTED_NAMESPACE get pvc -l release=production-postgresql)
    - if [[ -n "$pvc_after_delete" ]]; then echo "no postgresql pvc should be present"; exit 1; fi

test-delete-postgresql-failed:
  extends: test-deploy-postgres-enabled
  script:
    - auto-deploy download_chart
    - auto-deploy deploy
    # make sure that the helm release deployments always fails very fast
    - export POSTGRES_HELM_UPGRADE_EXTRA_ARGS="--timeout 1s"
    - export POSTGRES_VERSION=9.99.99
    - export AUTO_DEVOPS_ATOMIC_RELEASE=false
    # Deployment will fail, but we wnat to continue anyway and delete the failed application
    - auto-deploy deploy || failed_as_expected
    - helm get all production
    - helm get all production-postgresql
    - auto-deploy delete
    - helm get all production && expected_error || failed_as_expected
    - helm get all production-postgresql && expected_error || failed_as_expected
    - pvc_after_delete=$(kubectl -n $EXPECTED_NAMESPACE get pvc -l release=production-postgresql)
    - if [[ -n "$pvc_after_delete" ]]; then echo "no postgresql pvc should be present"; exit 1; fi


test-delete-canary-postgresql:
  extends: test-deploy-postgres-enabled
  script:
    - auto-deploy download_chart
    - auto-deploy deploy canary
    - helm get all production-canary
    - auto-deploy deploy
    - helm get all production
    - helm get all production-postgresql
    - auto-deploy delete canary
    - helm get all production-canary && expected_error || failed_as_expected
    - helm get all production
    - helm get all production-postgresql

test-chart-major-version-upgrade:
  extends: test-deploy
  script:
    - auto-deploy initialize_tiller
    # Copying bundled chart from local storage and the deployment should succeed
    - auto-deploy download_chart
    - auto-deploy deploy
    # Modifying the chart version and the deployment should fail
    - "sed -i 's/version:.*/version: 10.0.0/g' chart/Chart.yaml"
    - cat chart/Chart.yaml
    - auto-deploy deploy| tee deploy.log || true
    - grep -q "Detected a major version difference" deploy.log || exit 1
    # Force deploy with the AUTO_DEVOPS_FORCE_DEPLOY option and the deployment should succeed
    - export AUTO_DEVOPS_FORCE_DEPLOY_V10=true
    - auto-deploy deploy| tee deploy.log
    - grep -q "allowed to force deploy" deploy.log || exit 1

rspec:
  extends:
    - .rules:except-docs
  stage: test
  image: ruby:2.7
  before_script:
    - gem install --no-document rspec
  script:
    - rspec test/rspec

commitlint:
  stage: test
  image: node:12
  needs: []
  before_script:
    - npm install
  script:
    - npx --quiet commitlint --from="$CI_MERGE_REQUEST_DIFF_BASE_SHA" --help-url 'https://gitlab.com/gitlab-org/cluster-integration/auto-deploy-image#git-commit-guidelines'
  rules:
    - if: "$CI_MERGE_REQUEST_DIFF_BASE_SHA"

# This complements the except-docs rule in rules.gitlab-ci.yml to ensure that do
# not skip tests when code is *actually* changed.
assert-docs-only:
  image: alpine
  rules:
    - if: '$CI_COMMIT_MESSAGE =~ /\Adocs:/'
  before_script:
    - apk add git
  script:
    - git diff --name-only HEAD~1 | grep -v '.md$' && exit 1 || exit 0
